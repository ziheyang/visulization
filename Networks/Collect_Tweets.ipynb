{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "import json\n",
    "import sys\n",
    "#import langid\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "consumer_key = 'OrBHhvBsTwtzKPIOJ0XFjCCLw'\n",
    "consumer_secret = 'aoisrrqSka7ZirLjjwM5A4y0y4ZySuA62C782WSpEBANLKN603'\n",
    "access_token = '996280787971391489-n3wdQOJe6SflAqDbutSlhRQi7FoDfjU'\n",
    "access_secret = 'q17dGaIzpLj3FLQeE10dqSpJa7LN0z2RpR0bPtZOEY556'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# do not want retweet\n",
    "def analyze_status(text):\n",
    "    if 'RT' in text[0:3]:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "# get the data\n",
    "class MyStreamListener(StreamListener):\n",
    "    tweet_number = 0\n",
    "    def on_status(self, status):\n",
    "        if 'sex' not in status.text and 'Sex' not in status.text and not analyze_status(status.text):\n",
    "            with open('tweets.txt','a') as tf:\n",
    "                tf.write(str(status.text))\n",
    "                self.tweet_number += 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if self.tweet_number>=5000:\n",
    "            sys.exit(1)\n",
    "    def on_error(self, status):\n",
    "        print(\"Error Code : \" + status)\n",
    "\n",
    "hashlist = ['#TikTok', '#tiktok', 'TikTok', 'tiktok', 'TIKTOK'] \n",
    "\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener , tweet_mode='extended')\n",
    "\n",
    "\n",
    "myStream.filter(languages=['en'],track=hashlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(\"tweets.txt\") as f:\n",
    "    # read each line\n",
    "    flines = f.readlines()\n",
    "    for line in flines:\n",
    "        #tweets.append({\"text\":line})\n",
    "        tweets.append(line)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # Map POS tag to first character lemmatize()\n",
    "#     print(\"test\")\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     print(\"test1\")\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "punctuation = punctuation + \"…\" + \"‼\"\n",
    "stopwords_list = set(stopwords.words('english') + [\"follow\"])\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_step1(tweet):\n",
    "    tweet = re.sub(r'^RT\\b', '', tweet) # remove RT at the begininig of sentense\n",
    "    tweet = re.sub(r'(?:&[\\w_]+;)', '', tweet) # remove &amp;\n",
    "    tweet = tweet.lower() # convert text to lower-case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) # remove URLs \n",
    "    tweet = re.sub(r'(?:#[\\w_]+)', '', tweet)# remove hashtag\n",
    "    tweet = re.sub(r'<.*?>', '', tweet) # remove HTML\n",
    "    tweet = re.sub(r'(?:@[\\w_]+)', '', tweet) # remove @-mentions \n",
    "    return tweet\n",
    "\n",
    "def clean_step2(tweet):\n",
    "    tweet = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', '', tweet) # remove numbers\n",
    "    #tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.encode(\"ascii\", errors=\"ignore\").decode() # remove non-English words\n",
    "    word_list = tweet.split(\" \")\n",
    "    word_list = [word for word in word_list if word not in stopwords_list] #remove stopwords\n",
    "    tweet = \" \".join(word_list)\n",
    "    #tweet = emoji.emojize(tweet)\n",
    "    return tweet\n",
    "\n",
    "def clean_step3(tweet):\n",
    "    word_list = tweet.split(\" \")\n",
    "    word_list = [porter.stem(word) for word in word_list] #stemming\n",
    "    word_list = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list if w != \"\"] #lemmatizer\n",
    "    word_list = [word for word in word_list if len(word) > 1] #remove single-char words\n",
    "    word_list = [word for word in word_list if word != \"tiktok\"] \n",
    "    tweet = \" \".join(word_list)\n",
    "    pattern = r\"[{}]\".format(punctuation) # create the punctuations pattern\n",
    "    tweet = re.sub(pattern, \"\", tweet) \n",
    "    return tweet\n",
    "\n",
    "\n",
    "processed_tweets=[]\n",
    "list_tweets = []\n",
    "    \n",
    "for tweet in tweets:\n",
    "    result = clean_step1(tweet)\n",
    "    if result == \"\":\n",
    "        continue\n",
    "    list_tweets.append(result)\n",
    "    processed_tweets.append({\"text\":result})\n",
    "    \n",
    "tweets = list_tweets\n",
    "  \n",
    "processed_tweets=[]\n",
    "list_tweets = []\n",
    "    \n",
    "for tweet in tweets:\n",
    "    result = clean_step2(tweet)\n",
    "    if result == \"\":\n",
    "        continue\n",
    "    list_tweets.append(result)\n",
    "    processed_tweets.append({\"text\":result})\n",
    "    \n",
    "tweets = list_tweets\n",
    "\n",
    "processed_tweets=[]\n",
    "list_tweets = []\n",
    "    \n",
    "for tweet in tweets:\n",
    "    result = clean_step3(tweet)\n",
    "    if result == \"\":\n",
    "        continue\n",
    "    list_tweets.append(result)\n",
    "    processed_tweets.append({\"text\":result})\n",
    "    \n",
    "    \n",
    "processed_tweets\n",
    "\n",
    "cleaned_df = pd.DataFrame.from_dict(processed_tweets)\n",
    "\n",
    "\n",
    "out_csv = []\n",
    "for t in list_tweets:\n",
    "    out_csv.append(t.split())\n",
    "    \n",
    "import csv\n",
    "\n",
    "with open(\"tweets.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
